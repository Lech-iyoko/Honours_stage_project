# -*- coding: utf-8 -*-
"""Face_recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZzwhBIZThED2Ch2MBqMuJ9IYqUZvIlB
"""

# STEPS FOR DEVELOPING FACE RECOGNITION MODEL

# Install required depencies

# Loading both custom and online datasets and annotations

# Data pre-processing

# Define my face recognition model using MobileNet_V2

# Add layers for classification and fine-tuning as needed

# Define data augmentation and preprocessing

# Train my face recognition model using filtered_train_image_paths

# Evaluate my model on filtered_val_image_paths

# Deploy my model for simulated smart door access control or AGV face recognition

# Display results as per my project's requirements

!pip install labelme opencv-python albumentations tensorflow matplotlib

!pip install dropbox

# Import necessary libraries
import os
import tempfile
import tensorflow as tf
import os
import cv2
import dropbox
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator     # for automatic loading and pre-processing
from tensorflow.keras.preprocessing.image import load_img

print(tf.__version__)

# Show the Images in my dropbox files that will be used to train the model

# Authenticate Dropbox API using your access token
access_token = ''
dbx = dropbox.Dropbox(access_token)

# Define WIDER Face datasets image folder path
image_folder_path = '/Dataset/WIDER Face dataset/WIDER_train/WIDER_train/images'

# Define custom datasets image folder path
custom_img_folder_path =  '/Dataset/OneDrive_2024-03-07/Custom dataset'

# List contents of the image folder
for entry in dbx.files_list_folder(image_folder_path).entries:
    print(entry.name)

# List contents of the custom image folder
for entry in  dbx.files_list_folder(custom_img_folder_path).entries:
  print(entry.name)


# Print current working directory for verification
print(f"Current working directory: {os.getcwd()}")

# Load WIDER Face annotations paths
train_annotations_path_one = '/Dataset/WIDER Face dataset/wider_face_train_bbx_gt.txt'
val_annotations_path_one = '/Dataset/WIDER Face dataset/wider_face_val_bbx_gt.txt'

# Load custom dataset annotation path
custom_annotations_directory_path = '/Dataset/OneDrive_2024-03-07/Custom dataset/Annotation/task_label_customdataset_annotations_2024_03_09_20_09_38_pascal voc 1.1/Annotations'

# Print the path for verification
print(f"Using custom annotations directory: {custom_annotations_directory_path}")

# List contents of the custom annotation directory using Dropbox API
print("Contents of custom annotation directory:")
try:
    for entry in dbx.files_list_folder(custom_annotations_directory_path).entries:
        print(entry.name)
except dropbox.exceptions.ApiError as e:
    print(f"Error: {e}")

local_directory = '/content'

# Download the WIDER Face annotations
wider_train_annotations_local_path = os.path.join(local_directory, 'wider_face_train_bbx_gt.txt')
wider_val_annotations_local_path = os.path.join(local_directory, 'wider_face_val_bbx_gt.txt')

with open(wider_train_annotations_local_path, 'wb') as f:
    metadata, res = dbx.files_download(path=train_annotations_path_one)
    f.write(res.content)

with open(wider_val_annotations_local_path, 'wb') as f:
    metadata, val_res = dbx.files_download(path=val_annotations_path_one)
    f.write(val_res.content)

# Download the custom dataset annotations path
custom_annotations_directory_path = '/Dataset/OneDrive_2024-03-07/Custom dataset/Annotation/task_label_customdataset_annotations_2024_03_09_20_09_38_pascal voc 1.1/Annotations'

local_custom_annotations_directory = os.path.join(local_directory, 'Custom dataset labels', 'Annotations')

# Create the local directory for custom annotations if it doesn't exist
os.makedirs(local_custom_annotations_directory, exist_ok=True)

# Iterate over each file in the Annotations directory and download it
try:
    for entry in dbx.files_list_folder(custom_annotations_directory_path).entries:
        file_path = entry.path_display
        file_name = os.path.basename(file_path)
        local_file_path = os.path.join(local_custom_annotations_directory, file_name)

        with open(local_file_path, 'wb') as f:
            metadata, custom_res = dbx.files_download(path=file_path)
            f.write(custom_res.content)
except Exception as e:
    print(f"Error: {e}")

# Print the local directory paths
print("Local custom annotations directory:", local_custom_annotations_directory)

# Download the custom dataset image path
custom_dataset_directory_path = '/Dataset/OneDrive_2024-03-07/Custom dataset/All'

local_custom_dataset_directory = os.path.join(local_directory  , 'Custom dataset images', 'Images')

# Create the local directory for the custom dataset images if it doesn't exist
os.makedirs(local_custom_dataset_directory, exist_ok=True)

# iterate over each file in the All(custom images) directory and download it
try:
    for entry in dbx.files_list_folder(custom_dataset_directory_path).entries:
        file_path = entry.path_display
        file_name = os.path.basename(file_path)
        local_file_path = os.path.join(local_custom_dataset_directory, file_name)

        with open(local_file_path, 'wb') as f:
            metadata, custom_res = dbx.files_download(path=file_path)
            f.write(custom_res.content)
except Exception as e:
        print(f"Error: {e}")

# Print the local directory paths
print("Local custom dataset directory:", local_custom_dataset_directory)

# Define the Dropbox path for WIDER Face images
wider_images_directory_path = '/Dataset/WIDER Face dataset/WIDER_train/WIDER_train/images_2'

# Define the local directory path for WIDER Face images
local_wider_images_directory = os.path.join(local_directory, 'WIDER Face Images', 'Images')

# Create the local directory for the WIDER Face images if it doesn't exist
os.makedirs(local_wider_images_directory, exist_ok=True)


# Iterate over each file in the WIDER Face images directory and download it
try:
    for entry in dbx.files_list_folder(wider_images_directory_path).entries:
        file_path = entry.path_display
        file_name = os.path.basename(file_path)
        local_file_path = os.path.join(local_wider_images_directory, file_name)

        with open(local_file_path, 'wb') as f:
            metadata, wider_res = dbx.files_download(path=file_path)
            f.write(wider_res.content)
except Exception as e:
    print(f"Error: {e}")

# Print the local directory path for the WIDER Face images
print("Local WIDER Face images directory:", local_wider_images_directory)

# List contents of the custom dataset directory in Dropbox
print("Contents of custom dataset directory in Dropbox:")
try:
    for entry in dbx.files_list_folder(custom_dataset_directory_path).entries:
        print(entry.name)
except dropbox.exceptions.ApiError as e:
    print(f"Error: {e}")

# Import necessary libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout,Flatten,Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from PIL import Image

# Filter image paths based on keywords
def filter_image_paths(image_folder_path, keywords, custom_img_folder_path=None):
    filtered_image_paths = []

    # Check if custom_img_folder_path is provided
    if custom_img_folder_path:
        # List files in the custom image folder
        for entry in dbx.files_list_folder(custom_img_folder_path).entries:
            if any(keyword.lower() in entry.name.lower() for keyword in keywords):
                filtered_image_paths.append(entry.name)

    # List files in the image folder
    for entry in dbx.files_list_folder(image_folder_path).entries:
        if any(keyword.lower() in entry.name.lower() for keyword in keywords):
            filtered_image_paths.append(entry.name)

    # Access the length of the list to get the number of filtered paths
    number_of_filtered_paths = len(filtered_image_paths)
    print(f"Number of filtered image paths: {number_of_filtered_paths}")

    return filtered_image_paths


# Define keywords for filtering
your_keywords = ['press_conference', 'election_campaign', 'cheering', 'street_battle', 'swimming', 'angler', 'voter', 'sports_coach_trainer', 'row_boat', 'running', 'football', 'soccer', 'All']

# Get filtered image paths
filtered_image_paths = filter_image_paths(image_folder_path, your_keywords, custom_img_folder_path)
print(filtered_image_paths)

# Define image dimensions
img_rows, img_cols = 224, 224

# Load MobileNetV2 architecture
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))

# Freeze the base model layers
base_model.trainable = False

# Define ratio for training and validation
train_ratio = 0.8
num_train_images = int(train_ratio * len(filtered_image_paths))

# Add custom classification head for face recognition
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(num_train_images, activation='softmax')(x)

# Define the model architecture
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_train_images, activation='softmax')
])

# Compile the model
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Define the directory paths for training and validation data
custom_data_dir = '/content/Custom dataset images/Images'

# Print sample image shape (before rescaling)
sample_image_path_custom = os.path.join(custom_data_dir, 'Imaan.JPG')
sample_image_custom = Image.open(sample_image_path_custom)
print("Sample Image Shape (Before Rescaling):", sample_image_custom.size)


WIDER_data_dir = '/content/WIDER Face Images/Images'
# Print sample image shape (before rescaling)
sample_image_path_wider = os.path.join(WIDER_data_dir, '3_Riot_Riot_3_1006.jpg')
sample_image_wider = Image.open(sample_image_path_wider)
print("Sample Image Shape (Before Rescaling):", sample_image_wider.size)


# Combine the directories for training data
train_data_dirs = [custom_data_dir, WIDER_data_dir]

print(train_data_dirs)

def preprocess_images(train_data_dirs, img_rows, img_cols, batch_size):
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    train_generator = train_datagen.flow_from_directory(
        train_data_dirs,
        target_size=(img_rows, img_cols),
        batch_size=batch_size,
        class_mode='sparse'
    )

    return train_generator

import shutil

def combine_directories(train_data_dirs, combined_data_dir):
    # Create the combined data directory if it doesn't exist
    os.makedirs(combined_data_dir, exist_ok=True)

    # Iterate over each class directory
    for class_index, directory in enumerate(train_data_dirs, start=1):
        # Create a subdirectory for the class
        class_dir = os.path.join(combined_data_dir, f'class_{class_index}')
        os.makedirs(class_dir, exist_ok=True)

        # Copy images from the class directory to the combined directory
        image_files = os.listdir(directory)
        for filename in image_files:
            shutil.copy(os.path.join(directory, filename), class_dir)

    return combined_data_dir

combined_data_dir = '/content/Combined_datasets/Combined_images'
combine_directories(train_data_dirs, combined_data_dir)

# List the contents of the combined directory
contents = os.listdir(combined_data_dir)

# Print the contents
print("Contents of the combined directory:")
for item in contents:
    print(item)

# Define batch size and number of epochs
batch_size = 32
epochs = 10

# Preprocess the images
train_generator = preprocess_images(combined_data_dir, img_rows, img_cols, batch_size)

# Initialize ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1./255)

# Data Augmentation and preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2)  # Use validation_split if you want to split your data into training and validation

train_generator = train_datagen.flow_from_directory(
    combined_data_dir,
    target_size=(img_cols, img_rows),
    batch_size=batch_size,
    class_mode='sparse',
    subset='training')  # Specify 'training' subset for training generator

validation_generator = train_datagen.flow_from_directory(
    combined_data_dir,
    target_size=(img_cols, img_rows),
    batch_size=batch_size,
    class_mode='sparse',
    subset='validation')  # Specify 'validation' subset for validation generator

# Print sample batch images and labels shape
sample_batch_images, sample_batch_labels = train_generator.next()
print("Sample Batch Images Shape:", sample_batch_images.shape)
print("Sample Batch Labels Shape:", sample_batch_labels.shape)

# Print number of steps per epoch for training and validation
steps_per_epoch_train = len(train_generator)
steps_per_epoch_val = len(validation_generator)
print("Steps per Epoch (Training):", steps_per_epoch_train)
print("Steps per Epoch (Validation):", steps_per_epoch_val)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    callbacks=[early_stopping]
)

# Evaluate Your Model
loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')

# Save the model
model.save('face_recognition_model.keras')

# Get the current working directory
current_directory = os.getcwd()

# Define the path where the model is saved
model_path = os.path.join(current_directory, 'face_recognition_model.h5')

# Print the path
print("Model saved at:", model_path)

# Section for real time demonstration with laptop camera

from keras.models import load_model
from keras.applications.mobilenet_v2 import preprocess_input
from google.colab.patches import cv2_imshow

model = load_model('face_recognition_model.keras')

model.summary()

# Load the label names
target_names = ['Lech', 'Alex', 'Imaan', 'Nick']

# Load the face cascade classifier
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Open the webcam
cap = cv2.VideoCapture(0)

# Set the confidence threshold for predictions
confidence_threshold = 0.8

while True:
    # Capture frame-by-frame
    ret, frame = cap.read()

    # Add debug statements
    print("Ret:", ret)
    print("Frame:", frame)

    # Detect faces in the frame
    faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    for (x, y, w, h) in faces:
        # Extract the face region
        face = frame[y:y+h, x:x+w]

        # Resize the face to match the model's input size
        face_resized = cv2.resize(face, (160, 160))

        # Preprocess the face image for the model
        face_resized = np.expand_dims(face_resized, axis=0) / 255.0

        # Perform face recognition using the model
        embedding = model.predict(face_resized)
        predicted_class = np.argmax(embedding)
        confidence = np.max(embedding)

        # Check if confidence is above the threshold
        if confidence > confidence_threshold and predicted_class < len(target_names):
            predicted_name = target_names[predicted_class]
        else:
            predicted_name = 'Unknown'

        # Draw bounding box around the face and label it with confidence
        label_text = '{} ({:.2f})'.format(predicted_name, confidence)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(frame, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Display the frame
    cv2_imshow(frame)

    # Press 'q' to exit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the capture and close windows
cap.release()
cv2.destroyAllWindows()